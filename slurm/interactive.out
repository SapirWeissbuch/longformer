GPU available: True, used: True
TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0]
Using native 16bit precision.
/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: You have defined a `val_dataloader()` and have defined a `validation_step()`, you may also want to define `validation_epoch_end()` for accumulating stats.
  warnings.warn(*args, **kwargs)
/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.
  warnings.warn(*args, **kwargs)
Set SLURM handle signals.

  | Name                 | Type       | Params
----------------------------------------------------
0 | model                | Longformer | 148 M 
1 | qa_outputs           | Linear     | 1 K   
2 | learned_weighted_sum | Linear     | 3     
Loaded model with config:
RobertaConfig {
  "attention_dilation": [
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1
  ],
  "attention_mode": "tvm",
  "attention_probs_dropout_prob": 0.1,
  "attention_window": [
    256,
    256,
    256,
    256,
    256,
    256,
    256,
    256,
    256,
    256,
    256,
    256
  ],
  "autoregressive": false,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "ignore_attention_mask": false,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 4098,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

Namespace(attention_mode='sliding_chunks', attention_window=256, batch_size=1, dev_dataset='data/squad-wikipedia-dev-4096-duplicates.json', disable_checkpointing=False, doc_stride=-1, epochs=1, fp32=False, gpus=1, ignore_seq_with_no_answers=False, lr=3e-05, max_answer_length=30, max_doc_len=4096, max_num_answers=64, max_question_len=55, max_seq_len=512, model_path='longformer_model/longformer-base-4096', n_best_size=20, no_progress_bar=False, num_workers=1, regular_softmax_loss=False, resume_ckpt=None, save_dir='triviaqa', save_prefix='interactive', seed=4321, seq2seq=False, test=False, train_dataset='data/squad-wikipedia-train-4096-duplicates.json', val_every=0.5, val_percent_check=1.0, warmup=1000)
>>>>>>> #steps: 110648.0, #epochs: 1, batch_size: 1 <<<<<<<
reading file: data/squad-wikipedia-dev-4096-duplicates.json
done reading file: data/squad-wikipedia-dev-4096-duplicates.json
Validation sanity check: 0it [00:00, ?it/s]/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Validation sanity check:  50%|█████     | 1/2 [00:02<00:02,  2.93s/it]Validation sanity check: 100%|██████████| 2/2 [00:03<00:00,  1.27s/it]before sync --> sizes: 2, 2, 2, 2
after sync --> sizes: 2, 2, 2, 2
                                                                      reading file: data/squad-wikipedia-train-4096-duplicates.json
done reading file: data/squad-wikipedia-train-4096-duplicates.json
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/139105 [00:00<?, ?it/s]Epoch 1:   0%|          | 0/139105 [00:00<?, ?it/s] /cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/scripts/triviaqa.py:512: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)
  eos_token_indices = (input_ids == self.tokenizer.eos_token_id).nonzero()
/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/scripts/triviaqa.py:514: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert eos_token_indices.size(0) == 2 * input_ids.size(0)
/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/scripts/triviaqa.py:515: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert eos_token_indices.size(1) == 2
Traceback (most recent call last):
  File "/usr/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/interactive_longformer/interactive_longformer.py", line 469, in <module>
    main(args)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/interactive_longformer/interactive_longformer.py", line 461, in main
    trainer.fit(model)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1003, in fit
    results = self.single_gpu_train(model)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py", line 186, in single_gpu_train
    results = self.run_pretrain_routine(model)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1213, in run_pretrain_routine
    self.train()
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 370, in train
    self.run_training_epoch()
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 452, in run_training_epoch
    batch_output = self.run_training_batch(batch, batch_idx)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 632, in run_training_batch
    self.hiddens
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 773, in optimizer_closure
    opt_idx, hiddens)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 946, in training_forward
    output = self.model.training_step(*args)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/interactive_longformer/interactive_longformer.py", line 348, in training_step
    self.logger.experiment.add_graph(InteractiveTriviaQA(self.args, self.current_interaction_num, self.max_num_of_interactions), sample_data)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py", line 714, in add_graph
    self._get_file_writer().add_graph(graph(model, input_to_model, verbose))
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/utils/tensorboard/_pytorch_graph.py", line 291, in graph
    raise e
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/utils/tensorboard/_pytorch_graph.py", line 285, in graph
    trace = torch.jit.trace(model, args)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/jit/__init__.py", line 955, in trace
    check_tolerance, strict, _force_outplace, _module_class)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/jit/__init__.py", line 1109, in trace_module
    module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, strict, _force_outplace)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 720, in _call_impl
    result = self._slow_forward(*input, **kwargs)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 704, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/interactive_longformer/interactive_longformer.py", line 237, in forward
    sequence_output = self.model(input_ids, attention_mask=attention_mask)[0]
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 720, in _call_impl
    result = self._slow_forward(*input, **kwargs)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 704, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/transformers/modeling_bert.py", line 825, in forward
    input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 720, in _call_impl
    result = self._slow_forward(*input, **kwargs)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 704, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/transformers/modeling_roberta.py", line 82, in forward
    input_ids, token_type_ids=token_type_ids, position_ids=position_ids, inputs_embeds=inputs_embeds
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/transformers/modeling_bert.py", line 207, in forward
    inputs_embeds = self.word_embeddings(input_ids)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 720, in _call_impl
    result = self._slow_forward(*input, **kwargs)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 704, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/lib/python3.7/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select
Loaded model with config:
RobertaConfig {
  "attention_dilation": [
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1
  ],
  "attention_mode": "tvm",
  "attention_probs_dropout_prob": 0.1,
  "attention_window": [
    256,
    256,
    256,
    256,
    256,
    256,
    256,
    256,
    256,
    256,
    256,
    256
  ],
  "autoregressive": false,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "ignore_attention_mask": false,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 4098,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select
Error occurs, No graph saved
