# Cheatsheet for Interactive Project

* All scripts should be run from project's main directory ("longformer")

## Setup
1. Follow the setup instractions from the README.
2. Go to longformer/scripts/cheatsheet
3. Run the instructions listed in ----- Instructions to train TriviaQA ----- section

(
### ----- Instructions to train TriviaQA -----
# Relevant files:
# - scripts/triviaqa.py - our training code implemented in pytorch-lightning
# - scripts/triviaqa_utils - copied from https://github.com/mandarjoshi90/triviaqa with slight modifications

# Convert to a squad-like format. This is slighlty modified from the official scripts
# here https://github.com/mandarjoshi90/triviaqa/blob/master/utils/convert_to_squad_format.py
# to keep all answers in the document, not just the first answer. It also added the list of
# textual answers to make evaluation easy.
python -m scripts.triviaqa_utils.convert_to_squad_format  \
  --triviaqa_file path/to/qa/wikipedia-dev.json  \
  --wikipedia_dir path/to/evidence/wikipedia/   \
  --web_dir path/to/evidence/web/  \
  --max_num_tokens 4096  \   # only keep the first 4096 tokens
  --squad_file path/to/output/squad-wikipedia-dev-4096.json
)

### Train original triviaqa longformer model (reproduce results from Longformer's paper):
python -m scripts.triviaqa  \
    --train_dataset data/squad-wikipedia-train-4096.json  \
    --dev_dataset data/squad-wikipedia-dev-4096.json  \
    --gpus 1  --batch_size 8  --num_workers 4 \
    --lr 0.00003 --warmup 1000 --epochs 4  --max_seq_len 4096 --doc_stride -1  \
    --save_prefix original  \
    --model_path longformer_model/longformer-large-4096  --seed 4321 \
    --project_name "Question Paraphrases" \
    --run_name "Original TriviaQA model" \
    --predictions_fn predictions/original_triviaqa_model.json \

### Creating data for interactive training:
python -m interactive_longformer.data_processing.process_for_interactive_training \
        --inp_fn=data/squad-wikipedia-dev-4096.json \
        --out_fn=data/pegasus-5-squad-wikipedia-dev-4096.json \
        --max_num_of_paraphrases=5 \
        --seed=42 --mode=pegasus --batch_size=200

(mode can be pegasus/duplicates/back-translate)

(same with train data)

### Interactive training script:
python -m interactive_longformer.interactive_longformer  \
    --train_dataset data/partitioned_5_squad-wikipedia-train-4096.json  \
    --dev_dataset data/partitioned_5_squad-wikipedia-dev-4096.json  \
    --gpus 1  --batch_size 8  --num_workers 4 \
    --lr 0.00003 --warmup 1000 --epochs 4  --max_seq_len 4096 --doc_stride -1  \
    --save_prefix original  \
    --model_path longformer_model/longformer-large-4096  --seed 4321 \
    --project_name "Enter Project Name" \
    --run_name "Enter Run Name" \
    --total_interactions_num 5 \
    --predictions_fn predictions/filename \


### Example slurm sbatch file:
#!/bin/bash
#SBATCH --mem=32g
#SBATCH -c1
#SBATCH --time=2-0
#SBATCH --gres=gpu:1,vmem:32g
#SBATCH --output=/cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/slurm/train_back_translate.out
module load cuda/10.2
source /cs/labs/gabis/sapirweissbuch/projects/TeacherFeedbackTrainingProject/longformer/env/bin/activate
python -m interactive_longformer.interactive_longformer  \
    --train_dataset data/back-translate-5-squad-wikipedia-train-4096.json  \
    --dev_dataset data/back-translate-5-squad-wikipedia-dev-4096.json  \
    --gpus 1  --batch_size 8  --num_workers 4 \
    --lr 0.00003 --warmup 1000 --epochs 4  --max_seq_len 1024 --doc_stride -1  \
    --save_prefix interactive  \
    --model_path longformer_model/longformer-base-4096  --seed 4321 \
    --project_name "Question Paraphrases" \
    --run_name "Back Translate Interactive Training" \
    --total_interactions_num 5 \
    --predictions_fn predictions/back_translate_interactive_training.json \
